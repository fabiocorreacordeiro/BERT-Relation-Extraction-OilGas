{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2db751bf-45e5-4304-92b7-bab4044a4a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Senha:  ········\n"
     ]
    }
   ],
   "source": [
    "# Configurando Proxy\n",
    "\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "\n",
    "chave  = os.getenv('USER')\n",
    "senha  = getpass('Senha: ')\n",
    "\n",
    "os.environ['HTTP_PROXY']  = f'http://{chave}:{senha}@inet-sys.petrobras.com.br:804'\n",
    "os.environ['HTTPS_PROXY'] = f'http://{chave}:{senha}@inet-sys.petrobras.com.br:804'\n",
    "os.environ['NO_PROXY']    = '127.0.0.1, localhost, petrobras.com.br, petrobras.biz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f705d95-26d1-42af-b187-efef4959ada4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://nexus.petrobras.com.br/nexus/repository/pypi-all/simple/\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.6/site-packages (3.6.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/site-packages (from nltk) (6.7)\n",
      "Requirement already satisfied: regex in /usr/local/lib64/python3.6/site-packages (from nltk) (2020.11.13)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/site-packages (from nltk) (4.60.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09d49446-97cd-4866-8e69-dcf3974a5180",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/UPE2/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/UPE2/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from rdflib import Graph\n",
    "from rdflib import URIRef\n",
    "import pickle\n",
    "import random\n",
    "from multiprocessing import Pool\n",
    "from spacy_langdetect import LanguageDetector\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d4c090-3c3b-4e97-9193-c3395a7d48be",
   "metadata": {},
   "source": [
    "### Identificanco potenciais entidades mencionadas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc532d1-8a94-43d8-b4e2-0c59466cb63e",
   "metadata": {},
   "source": [
    "Separando potenciais entidades mencionadas do domínio de Óleo e Gás (Taxonomia da Wograine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f38e8d30-eacb-4df1-90be-3b9e0b8154ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/KnowledgeGraph/Taxonomia_O&G.html\") as fp:\n",
    "    soup = BeautifulSoup(fp, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7931a56-753e-4415-97ab-dfc254373e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7008/7008 [00:00<00:00, 229008.37it/s]\n",
      "100%|██████████| 20786/20786 [00:00<00:00, 229375.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# Fazendo o parser no documento html\n",
    "\n",
    "EM_taxonomia = []\n",
    "\n",
    "for parent in tqdm(soup.find_all('h3'), total=len(soup.find_all('h3'))):\n",
    "    EM_taxonomia.append(re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", parent.get_text()).rstrip().lower())\n",
    "\n",
    "for parent in tqdm(soup.find_all('span'), total= len(soup.find_all('span'))):\n",
    "    EM_taxonomia.append(re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", parent.get_text()).rstrip().lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61d3d8da-a61b-49a7-a063-30086d6cb7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27794/27794 [00:00<00:00, 961883.93it/s]\n",
      "100%|██████████| 27861/27861 [00:02<00:00, 12768.89it/s]\n"
     ]
    }
   ],
   "source": [
    "# Limpando as Entidades Mencionadas provenientes da Taxonomia\n",
    "\n",
    "EM_taxonomia_new = []\n",
    "\n",
    "for em in tqdm(EM_taxonomia, total=len(EM_taxonomia)):\n",
    "    # Quebrando as entidades mencionadas separadas por ';'\n",
    "    em = em.split(';')\n",
    "    for new_em in em:\n",
    "        # Excluindo as entidades com apenas dois caracteres\n",
    "        if len(new_em.lstrip().rstrip()) > 2:\n",
    "            EM_taxonomia_new.append(new_em.lstrip().rstrip())\n",
    "\n",
    "EM_taxonomia = EM_taxonomia_new\n",
    "\n",
    "# Mantendo apenas as expressões multipalavras\n",
    "EM_taxonomia_new = []\n",
    "\n",
    "for e in tqdm(EM_taxonomia, total=len(EM_taxonomia)):\n",
    "    if len(word_tokenize(e, language='portuguese')) > 1:\n",
    "        EM_taxonomia_new.append(e)\n",
    "        \n",
    "EM_taxonomia = EM_taxonomia_new\n",
    "        \n",
    "# Eliminando os elementos duplicados\n",
    "EM_taxonomia = list(set(EM_taxonomia))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa579832-0246-4c0b-a91d-b43bfa5df4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potenciais entidades mencionadas proveniente da taxonomia:  21213\n"
     ]
    }
   ],
   "source": [
    "print(\"Potenciais entidades mencionadas proveniente da taxonomia: \", len(EM_taxonomia))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc173ae5-bee9-40c8-b2fc-d0e22db647ec",
   "metadata": {},
   "source": [
    "Conceitos e labels presentes no Knowledge Graph (Baseado principalmente no Thesauro do Petróleo da Universidade de Tulsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bcde566-a71c-4539-9d45-bad405e5ed23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=N351984d87d214055b42af3c7737220d8 (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Carregando Knowledge Graph\n",
    "g = Graph()\n",
    "g.parse(\"data/KnowledgeGraph/SKOS_Tulsa-e-instancias.nt\", format=\"turtle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb496e2b-dd48-4dbe-ad80-c67b1c5c2eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50866/50866 [00:00<00:00, 149161.91it/s]\n",
      "100%|██████████| 5391/5391 [00:00<00:00, 156131.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potenciais entidades mencionadas proveniente dos labels do KG:  56257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "EM_label = []\n",
    "\n",
    "g_rel = g.query( \n",
    "    \"\"\" \n",
    "    PREFIX skos: <http://www.w3.org/2004/02/skos/core#> \n",
    "    select ?o \n",
    "    where {\n",
    "        ?s \n",
    "        skos:prefLabel\n",
    "        ?o}  \n",
    "    \"\"\")\n",
    "\n",
    "for rel in tqdm(g_rel, total=len(g_rel)):\n",
    "    EM_label.append(rel[0].lower())\n",
    "\n",
    "g_rel = g.query( \n",
    "    \"\"\" \n",
    "    PREFIX skos: <http://www.w3.org/2004/02/skos/core#> \n",
    "    select ?o \n",
    "    where {\n",
    "        ?s \n",
    "        skos:altLabel\n",
    "        ?o}  \n",
    "    \"\"\")\n",
    "\n",
    "for rel in tqdm(g_rel, total=len(g_rel)):\n",
    "    EM_label.append(rel[0].lower())\n",
    "\n",
    "len(EM_label)\n",
    "print(\"Potenciais entidades mencionadas proveniente dos labels do KG: \", len(EM_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6506651c-1ddb-4e81-9291-4cffe4fbc0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25663/25663 [00:00<00:00, 62206.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potenciais entidades mencionadas proveniente dos conceitos do KG:  33070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "EM_concepts = []\n",
    "\n",
    "g_rel = g.query( \n",
    "    \"\"\" \n",
    "    PREFIX skos: <http://www.w3.org/2004/02/skos/core#> \n",
    "    select ?s ?o \n",
    "    where {\n",
    "        ?s \n",
    "        ?r\n",
    "        ?o}  \n",
    "    GROUP BY ?s\n",
    "    \"\"\")\n",
    "\n",
    "for rel in tqdm(g_rel, total=len(g_rel)):\n",
    "    \n",
    "    sub = rel[0].n3()\n",
    "    obj = rel[1].n3()\n",
    "    \n",
    "    if sub[:12] == '<http://bs/#':\n",
    "        EM_concepts.append(sub[12:-1].replace('+', ' ').lower())\n",
    "    if obj[:12] == '<http://bs/#':\n",
    "        EM_concepts.append(sub[12:-1].replace('+', ' ').lower())\n",
    "\n",
    "print(\"Potenciais entidades mencionadas proveniente dos conceitos do KG: \", len(EM_concepts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6a77df-385e-461a-983e-f7d9bb476a28",
   "metadata": {},
   "source": [
    "Juntando todas as potenciais entidades nomeadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4284e73-f5e3-4c1b-9879-ad83aebcbad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54541/54541 [00:00<00:00, 1332759.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potenciais entidades mencionadas total:  54417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "EM = EM_taxonomia + EM_label + EM_concepts\n",
    "#EM = EM_label + EM_concepts\n",
    "EM = list(set(EM))\n",
    "\n",
    "# instanciando as Stopwords em português e inglês\n",
    "stopwords_pt = nltk.corpus.stopwords.words('portuguese')\n",
    "stopwords_en = nltk.corpus.stopwords.words('english')\n",
    "stopwords = stopwords_pt + stopwords_en\n",
    "\n",
    "# Removendo as stopwords\n",
    "for stop in stopwords:\n",
    "    if stop in EM:\n",
    "        EM.remove(stop)\n",
    "        \n",
    "# Removendo Entidades 'estranhas' - Posteriormente deve ser revisto o Knowledge Graph\n",
    "# EE - lista de entidades estranhas\n",
    "ee = ['dois', 'três', 'quatro', 'exemplo', 'ser', 'valor', 'número', 'em geral',\n",
    "      'em relação', 'alto', 'baixo', 'inferior', 'pol', 'recente']\n",
    "\n",
    "for e in ee:\n",
    "    if e in EM:\n",
    "        EM.remove(e)\n",
    "\n",
    "# limpando as Entidades mencionadas com menos de dois caracteres \n",
    "EM_new = []\n",
    "for em in tqdm(EM, total=len(EM)):\n",
    "    # Excluindo as entidades com apenas dois caracteres\n",
    "    if len(em.lstrip().rstrip()) > 2:\n",
    "        EM_new.append(em.lstrip().rstrip())\n",
    "\n",
    "EM = list(set(EM_new))\n",
    "\n",
    "# Ordenando a lista por tamanho\n",
    "EM.sort(key=len, reverse=True)\n",
    "print(\"Potenciais entidades mencionadas total: \", len(EM))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd80d430-bb60-4634-bd56-eda45e0f3ad9",
   "metadata": {},
   "source": [
    "### Identificando as entidades no texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c0e4a63-a1ad-42ad-9a73-1e170de2c43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando os modelos spacy\n",
    "nlp_pt = spacy.load('pt_core_news_lg') \n",
    "nlp_en = spacy.load('en_core_web_lg') \n",
    "nlp_lang = nlp_en\n",
    "nlp_lang.add_pipe(LanguageDetector(), name='language_detector', last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d726a6f-6bb9-428d-afec-ae233feb86c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'teste'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Função para receber uma sentença como uma string e retornar no formato do dataset\n",
    "\n",
    "def sent_to_dataset(sent_test, EM_test=EM, nlp_lang = nlp_lang, nlp_pt=nlp_pt, nlp_en=nlp_en):\n",
    "    \n",
    "    # Verificando a língua da sentença (português ou inglês)   \n",
    "    doc = nlp_lang(sent_test)\n",
    "    lang_dic = doc._.language\n",
    "    \n",
    "    # Excluir sentenças com mais de 40 tokens\n",
    "    if len(doc) > 40:\n",
    "        return ([]) \n",
    "    \n",
    "    # Mantendo apenas as sentenças em português\n",
    "    if lang_dic['language'] != 'pt': # and lang_dic['language'] != 'en':\n",
    "        return ([])\n",
    "    else:\n",
    "        if lang_dic['language'] == 'pt':\n",
    "            if lang_dic['score'] < 0.95:\n",
    "                return ([])\n",
    "            nlp = nlp_pt\n",
    "            \n",
    "        #if lang_dic['language'] == 'en':\n",
    "        #    if lang_dic['score'] < 0.95:\n",
    "        #        return ([])\n",
    "        #    nlp = nlp_en\n",
    "    \n",
    "    # Verificar se a estrutura da sentença é Sujeito-Verbo-Objeto\n",
    "    sent_parser = nlp(sent_test)\n",
    "    subj = 0\n",
    "    root = 0\n",
    "    verb = ''\n",
    "    obj = 0\n",
    "\n",
    "    for span in sent_parser:\n",
    "        if span.dep_ == 'subj' or span.dep_ =='nsubj':\n",
    "            subj = subj + 1\n",
    "        if span.dep_ == 'obj' or span.dep_ =='dobj':\n",
    "            obj = obj + 1\n",
    "        if span.dep_ == 'ROOT':\n",
    "            root = root + 1\n",
    "            verb = span\n",
    "            \n",
    "    # Se a sentença não tiver exatamente um sujeito, um objeto e um verbo, função retorna vazio.    \n",
    "    if subj != 1 or obj != 1 or root != 1:\n",
    "        return ([])\n",
    "    \n",
    "    D = []\n",
    "    EM_sent = []\n",
    "    sent_test_lower = sent_test.lower()\n",
    "    sent_tok = word_tokenize(sent_test.lower(), language='portuguese')\n",
    "\n",
    "    # Loop para identificar se as Entidades Mencionadas estão presentes na sentença\n",
    "    for EM_t in EM_test:\n",
    "        \n",
    "        EM_ttok = word_tokenize(EM_t, language='portuguese')\n",
    "\n",
    "        #pos_n1 = []\n",
    "        for i in range(len(sent_tok) - len(EM_ttok) + 1):\n",
    "            if sent_tok[i:i+len(EM_ttok)] == EM_ttok:\n",
    "                #sent_test_lower = sent_test_lower.replace(EM_t, \"PAD \" * len(EM_ttok))\n",
    "                #sent_tok = word_tokenize(sent_test_lower, language='portuguese')\n",
    "                \n",
    "                # Substituinda as entidades identificadas por PAD\n",
    "                sent_tok[i:i+len(EM_ttok)] = [\"PAD\"] * len(EM_ttok)\n",
    "                EM_sent.append((EM_t, (i,i+len(EM_ttok)) ))\n",
    "                \n",
    "    # Criando dataset para sentenças com mais de duas entidades - Uma entrada para cada par de entidades\n",
    "\n",
    "    sent_tok = word_tokenize(sent_test.lower(), language='portuguese')\n",
    "    \n",
    "    if len(EM_sent) > 1:\n",
    "        \n",
    "        # iterando sobre as entidades identificadas\n",
    "        for n1 in range(len(EM_sent)-1):\n",
    "            for n2 in range(n1 + 1, len(EM_sent)):\n",
    "                # Verificando a ordem das entidades na frase\n",
    "                if EM_sent[n1][1][0] < EM_sent[n2][1][0]:\n",
    "                    #print(EM_sent[n1], EM_sent[n2])\n",
    "                    D.append(((word_tokenize(sent_test, \n",
    "                                             language='portuguese'),\n",
    "                               EM_sent[n1][1], \n",
    "                               EM_sent[n2][1]), \n",
    "                              EM_sent[n1][0], \n",
    "                              EM_sent[n2][0]))\n",
    "                          \n",
    "                else:\n",
    "                    #print(EM_sent[n2], EM_sent[n1])\n",
    "                    D.append(((word_tokenize(sent_test, \n",
    "                                             language='portuguese'),\n",
    "                               EM_sent[n2][1], \n",
    "                               EM_sent[n1][1]), \n",
    "                              EM_sent[n2][0], \n",
    "                              EM_sent[n1][0]))\n",
    "        \n",
    "    return (D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a76d967-0b8d-4dd5-b60b-2d7fb7c6882d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = ['Devido', 'às', 'condições', 'ambientais', 'existentes', 'na', 'Baía', 'da', 'Ilha', 'Grande', ',', 'o', 'crescimento', 'de', 'incrustações', 'desenvolve-se', 'com', 'extrema', 'rapidez', ',', 'criando', 'condições', 'propícias', 'para', 'um', 'estudo', 'nesse', 'campo', '.']\n",
    "#text = 'The figure 9 shows the effect of temperature in the corrosion dots, in 3% NaCl solution'\n",
    "text = 'A figura 9 ilustra o efeito da temperatura na corrosão por pites , em solução 3 % NaCI .'\n",
    "\n",
    "for i in sent_to_dataset(text):\n",
    "    print (i[1], i[0][1], '---', i[2], i[0][2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4f9d12-4c64-417d-81d2-09af5addf8ed",
   "metadata": {},
   "source": [
    "Carregando texto bruto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5fb8f05-7af7-4b06-9edf-9bef545fd49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1357 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subdir:  data/regis_abbyy_txt/boletins_tecnicos_OCR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1357/1357 [3:29:33<00:00,  9.27s/it]  \n"
     ]
    }
   ],
   "source": [
    "# Iterando por todos os arquivos da pasta raiz \"rootdir\"\n",
    "rootdir = 'data/regis_abbyy_txt/boletins_tecnicos_OCR'\n",
    "\n",
    "D = []\n",
    "\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    print('subdir: ', subdir)\n",
    "    \n",
    "    #progress_bar = tqdm(total=len(files))\n",
    "    for file in tqdm(files, total=len(files)):\n",
    "        \n",
    "        raw_text_path = os.path.join(subdir, file)\n",
    "        \n",
    "        with open(raw_text_path, \"r\", encoding=\"utf8\") as f:\n",
    "            text = f.read()\n",
    "            \n",
    "        # tokenizando e embaralhando as sentenças\n",
    "        sents = sent_tokenize(text, language='portuguese')\n",
    "        #random.shuffle(sents)\n",
    "\n",
    "        N = len(sents)\n",
    "        sents = sents[:N]\n",
    "        \n",
    "        # Paralelizando a tarefa\n",
    "        with Pool(processes=48) as pool:  \n",
    "            #progress_bar = tqdm(total=len(sents))\n",
    "            Dataset = list(pool.imap(sent_to_dataset, sents))\n",
    "#            Dataset = list(tqdm(pool.imap(sent_to_dataset, sents), total=len(sents), position=0, leave=True))\n",
    "        Dataset = [ent for sublist in Dataset for ent in sublist]\n",
    "        D = D + Dataset\n",
    "\n",
    "        #sublist = [sublist for sublist in Dataset]\n",
    "        #for data_list in sublist:\n",
    "        #    try:\n",
    "        #        D = D + data_list\n",
    "        #    except:\n",
    "        #        pass\n",
    "        \n",
    "        \n",
    "        #Save D at data folder\n",
    "        with open(\"data/D_boletins_tecnicos.pkl\", 'wb') as f:\n",
    "            # Pickle the 'data' dictionary using the highest protocol available.\n",
    "            pickle.dump(D, f, pickle.HIGHEST_PROTOCOL)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20089012-fb3e-4e90-8bb8-f5905a4995f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de pares de entidades encontradas:  41925\n"
     ]
    }
   ],
   "source": [
    "print ('Número de pares de entidades encontradas: ', len(D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02c883f0-3f8b-476e-a958-72cbd6d06278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((['Em',\n",
       "   'meados',\n",
       "   'do',\n",
       "   'Triássico',\n",
       "   ',',\n",
       "   'a',\n",
       "   'tectônica',\n",
       "   'extensional',\n",
       "   'gerou',\n",
       "   'espaço',\n",
       "   'de',\n",
       "   'alojamento',\n",
       "   'para',\n",
       "   'a',\n",
       "   'Formação',\n",
       "   'Santa',\n",
       "   'Maria',\n",
       "   '(',\n",
       "   '230-206',\n",
       "   'Ma',\n",
       "   ')',\n",
       "   'com',\n",
       "   'registro',\n",
       "   'fossilífero',\n",
       "   'do',\n",
       "   'Triássico',\n",
       "   'Médio',\n",
       "   '.'],\n",
       "  (11, 12),\n",
       "  (22, 24)),\n",
       " 'alojamento',\n",
       " 'registro fossilífero')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dbda83a9-f32e-4a3f-a922-71fe4c37002a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a84adad-6e0d-4bec-bebf-e0ef57aba2aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a685c80-ea05-4e6d-b804-43f9ee960d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_pt = spacy.load('pt_core_news_lg')\n",
    "nlp_pt.add_pipe(LanguageDetector(), name='language_detector', last=True)\n",
    "\n",
    "text = 'O Andar Alagoas caracteriza-se por empi-lhamento dominantemente retrogradacional nas bacias da margem leste brasileira , as quais são compostas por várias sequências deposicionais dominantemente transgressivas , predominando sedimentação continental nas margens e deposição evaporítica e marinha carbonático-siliciclástica nas porções mais distais ( Dias , 2004 ).'\n",
    "doc = nlp_pt(text)\n",
    "# document level language detection. Think of it like average language of the document!\n",
    "print(doc._.language)\n",
    "\n",
    "# sentence level language detection\n",
    "for sent in doc:\n",
    "    print(sent, '---', sent.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9a206147-d4b7-4255-8b01-0dd310673c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((['O',\n",
       "    'Andar',\n",
       "    'Alagoas',\n",
       "    'caracteriza-se',\n",
       "    'por',\n",
       "    'empi-lhamento',\n",
       "    'dominantemente',\n",
       "    'retrogradacional',\n",
       "    'nas',\n",
       "    'bacias',\n",
       "    'da',\n",
       "    'margem',\n",
       "    'leste',\n",
       "    'brasileira',\n",
       "    ',',\n",
       "    'as',\n",
       "    'quais',\n",
       "    'são',\n",
       "    'compostas',\n",
       "    'por',\n",
       "    'várias',\n",
       "    'sequências',\n",
       "    'deposicionais',\n",
       "    'dominantemente',\n",
       "    'transgressivas',\n",
       "    ',',\n",
       "    'predominando',\n",
       "    'sedimentação',\n",
       "    'continental',\n",
       "    'nas',\n",
       "    'margens',\n",
       "    'e',\n",
       "    'deposição',\n",
       "    'evaporítica',\n",
       "    'e',\n",
       "    'marinha',\n",
       "    'carbonático-siliciclástica',\n",
       "    'nas',\n",
       "    'porções',\n",
       "    'mais',\n",
       "    'distais',\n",
       "    '(',\n",
       "    'Dias',\n",
       "    ',',\n",
       "    '2004',\n",
       "    ')',\n",
       "    '.'],\n",
       "   (1, 3),\n",
       "   (27, 28)),\n",
       "  'andar alagoas',\n",
       "  'sedimentação'),\n",
       " ((['O',\n",
       "    'Andar',\n",
       "    'Alagoas',\n",
       "    'caracteriza-se',\n",
       "    'por',\n",
       "    'empi-lhamento',\n",
       "    'dominantemente',\n",
       "    'retrogradacional',\n",
       "    'nas',\n",
       "    'bacias',\n",
       "    'da',\n",
       "    'margem',\n",
       "    'leste',\n",
       "    'brasileira',\n",
       "    ',',\n",
       "    'as',\n",
       "    'quais',\n",
       "    'são',\n",
       "    'compostas',\n",
       "    'por',\n",
       "    'várias',\n",
       "    'sequências',\n",
       "    'deposicionais',\n",
       "    'dominantemente',\n",
       "    'transgressivas',\n",
       "    ',',\n",
       "    'predominando',\n",
       "    'sedimentação',\n",
       "    'continental',\n",
       "    'nas',\n",
       "    'margens',\n",
       "    'e',\n",
       "    'deposição',\n",
       "    'evaporítica',\n",
       "    'e',\n",
       "    'marinha',\n",
       "    'carbonático-siliciclástica',\n",
       "    'nas',\n",
       "    'porções',\n",
       "    'mais',\n",
       "    'distais',\n",
       "    '(',\n",
       "    'Dias',\n",
       "    ',',\n",
       "    '2004',\n",
       "    ')',\n",
       "    '.'],\n",
       "   (1, 3),\n",
       "   (11, 13)),\n",
       "  'andar alagoas',\n",
       "  'margem leste'),\n",
       " ((['O',\n",
       "    'Andar',\n",
       "    'Alagoas',\n",
       "    'caracteriza-se',\n",
       "    'por',\n",
       "    'empi-lhamento',\n",
       "    'dominantemente',\n",
       "    'retrogradacional',\n",
       "    'nas',\n",
       "    'bacias',\n",
       "    'da',\n",
       "    'margem',\n",
       "    'leste',\n",
       "    'brasileira',\n",
       "    ',',\n",
       "    'as',\n",
       "    'quais',\n",
       "    'são',\n",
       "    'compostas',\n",
       "    'por',\n",
       "    'várias',\n",
       "    'sequências',\n",
       "    'deposicionais',\n",
       "    'dominantemente',\n",
       "    'transgressivas',\n",
       "    ',',\n",
       "    'predominando',\n",
       "    'sedimentação',\n",
       "    'continental',\n",
       "    'nas',\n",
       "    'margens',\n",
       "    'e',\n",
       "    'deposição',\n",
       "    'evaporítica',\n",
       "    'e',\n",
       "    'marinha',\n",
       "    'carbonático-siliciclástica',\n",
       "    'nas',\n",
       "    'porções',\n",
       "    'mais',\n",
       "    'distais',\n",
       "    '(',\n",
       "    'Dias',\n",
       "    ',',\n",
       "    '2004',\n",
       "    ')',\n",
       "    '.'],\n",
       "   (11, 13),\n",
       "   (27, 28)),\n",
       "  'margem leste',\n",
       "  'sedimentação')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'O Andar Alagoas caracteriza-se por empi-lhamento dominantemente retrogradacional nas bacias da margem leste brasileira , as quais são compostas por várias sequências deposicionais dominantemente transgressivas , predominando sedimentação continental nas margens e deposição evaporítica e marinha carbonático-siliciclástica nas porções mais distais ( Dias , 2004 ).'\n",
    "sent_to_dataset(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
